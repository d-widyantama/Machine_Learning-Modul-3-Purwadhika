{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial features\n",
    "\n",
    "- Linear regression can be improved by adding polynomial features\n",
    "- Polynomial features are those features created by raising existing features to an exponent\n",
    "- For example, if we have a feature x, we can create a new feature x^2 by squaring x, x^3 by cubing x, and so on\n",
    "- This is useful when the relationship between the features and the target is nonlinear\n",
    "\n",
    "Polynomial features(3) = PolynomialFeatures(degree=3)\n",
    "\n",
    "- PolynomialFeatures(degree=d) transforms an array containing n features into an array containing (n+d)!/(d!n!) features, where n is the number of features in the original array\n",
    "- For example, if we have two features a and b, PolynomialFeatures(degree=2) will create the following features: 1, a, b, a^2, ab, b^2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ensemble methods**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods combine multiple machine learning models to create more powerful models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various Types: Voting & Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking method is a more advanced ensemble method that involves training a model to combine the predictions of several other models\n",
    "--> Base Learner & Meta Learner (Used to combine the predictions of the base learners) by default, the meta learner is a logistic regression model\n",
    "\n",
    "Voting method is a simple ensemble method in which several models are trained and each model predicts whether a data point belongs to the positive class or negative class; the predictions of all the models are then combined, and the combined prediction is used as the final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar Type: Tree Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models used in similar types are all tree-based models, which are models that use decision trees to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap Sampling: The process of creating a new dataset by sampling with replacement from the original dataset, allowing duplicate samples. n of bootstrap sample = n of models in ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging: Bootstrap Aggregating, a way to **decrease the variance** of a model by training many models on bootstrap samples of the original dataset and then averaging the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of Bagging are implemented in Random Forest, which is an ensemble method that uses a collection of decision trees that have been trained on bootstrap samples. In the end hard voting is used to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest Classifier. By convention, clf means 'Classifier'\n",
    "clf = RandomForestClassifier(n_estimators=2, random_state=0)\n",
    "#n_estimators=2 means 2 trees in the forest\n",
    "#random_state=0 means the random seed is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is an **iterative/sequential** method that attempts to reduce the bias of the combined models. Boosting methods train models sequentially, and each model attempts to correct the errors of the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting methods are more complex than bagging methods and usually perform better, but they are also more likely to overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform weak learner into strong learner (with the same models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Weak learner***: A model that is slightly better than random guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight concept: The weight of each data point **(features)** indicates how important the data point is to the model, and data points with higher weights are given more attention by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning in Boosting:\n",
    "- n_estimators: The number of models to iteratively train\n",
    "- shrinkage: The learning rate, which controls how much each model contributes to the overall ensemble\n",
    "- subsample: The fraction of data points to randomly sample for each model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods:\n",
    "- Gradient Boosting, in python: AdaBoost (Adaptive boosting)\n",
    "- Extreme Gradient Boosting, in python: XGBClassifier (XGBoost library), well-known for its speed and performance\n",
    "- CatBoost, in python: CatBoostClassifier \n",
    "\n",
    "All methods are based from Decision Tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
