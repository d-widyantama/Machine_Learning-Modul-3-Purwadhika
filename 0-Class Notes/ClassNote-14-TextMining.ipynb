{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thu, 12 Oct 2023\n",
    "# Text Mining\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Challenges of Text Mining: \n",
    "- **Unstructured** data\n",
    "- **High Dimensionality** (many features)\n",
    "- **Sparsity** (many zeros)\n",
    "- **Ambiguity** (many words have multiple meanings)\n",
    "- **Synonymy** (many words have similar meanings)\n",
    "- **Polysemy** (many words have multiple meanings)\n",
    "- **Collocation** (many words have different meanings when combined with other words)\n",
    "- **Domain-specific** (many words are specific to a domain)\n",
    "- **Dynamic** (language changes over time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sastrawi in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: nltk in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: nlp-id in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (0.1.15.0)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from nlp-id) (1.2.2)\n",
      "Requirement already satisfied: nltk==3.8.1 in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from nlp-id) (3.8.1)\n",
      "Requirement already satisfied: wget==3.2 in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from nlp-id) (3.2)\n",
      "Requirement already satisfied: pytest==7.3.1 in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from nlp-id) (7.3.1)\n",
      "Requirement already satisfied: click in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from nltk==3.8.1->nlp-id) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from nltk==3.8.1->nlp-id) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from nltk==3.8.1->nlp-id) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from nltk==3.8.1->nlp-id) (4.66.1)\n",
      "Requirement already satisfied: iniconfig in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from pytest==7.3.1->nlp-id) (2.0.0)\n",
      "Requirement already satisfied: packaging in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from pytest==7.3.1->nlp-id) (23.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from pytest==7.3.1->nlp-id) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from scikit-learn==1.2.2->nlp-id) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from scikit-learn==1.2.2->nlp-id) (1.11.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/Dwika/My Projects/.venv/lib/python3.11/site-packages (from scikit-learn==1.2.2->nlp-id) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Library\n",
    "\n",
    "!pip3 install sastrawi\n",
    "!pip3 install nltk\n",
    "!pip3 install nlp-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keywords**\n",
    "\n",
    "- Document: 1 or more paragraphs\n",
    "- Corpus: Collection of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case of Text Mining "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentiment Analysis\n",
    "- Topic Modeling\n",
    "- Text Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Tokenization\n",
    "\n",
    "Tokenization: Splitting a string into a list of words\n",
    "- Converting to lowercase / Case folding\n",
    "- Contractions / Normalization (e.g., \"don't\" to \"do not\")\n",
    "- Removing punctuation (?, , , !, etc.)\n",
    "- Removing numbers or converting numbers to words\n",
    "- Remove white spaces\n",
    "- Removing stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "sms = pd.read_csv('/Users/Dwika/My Projects/DATASETS/sms_spam_collection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('/Users/Dwika/My Projects/DATASETS/Data_berita.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul</th>\n",
       "      <th>kategori</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baekhyun EXO Berikan Contoh Baik Pencegahan Vi...</td>\n",
       "      <td>non_clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lee Seung Gi Akhirnya Beri Kabar Setelah Dicar...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UPDATE: Jaejoong JYJ Ngaku Kena Corona, Idol K...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Virus Corona Masuk ke Korea, Kenapa Banyak Ora...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16 Seleb yang Lagunya Bertema Virus Corona, Ad...</td>\n",
       "      <td>non_clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>Kemenkes Permudah Lansia dan Pelayan Publik Me...</td>\n",
       "      <td>non_clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>Satgas: Jakarta dan Jabar Penyumbang Kasus Cov...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>Awal Mula Varian Baru Virus Corona Masuk ke In...</td>\n",
       "      <td>non_clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>Jokowi Sebut Pengangguran di Indonesia Hampir ...</td>\n",
       "      <td>non_clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2029 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  judul       kategori\n",
       "0     Baekhyun EXO Berikan Contoh Baik Pencegahan Vi...  non_clickbait\n",
       "1     Lee Seung Gi Akhirnya Beri Kabar Setelah Dicar...      clickbait\n",
       "2     UPDATE: Jaejoong JYJ Ngaku Kena Corona, Idol K...      clickbait\n",
       "3     Virus Corona Masuk ke Korea, Kenapa Banyak Ora...      clickbait\n",
       "4     16 Seleb yang Lagunya Bertema Virus Corona, Ad...  non_clickbait\n",
       "...                                                 ...            ...\n",
       "2024  Kemenkes Permudah Lansia dan Pelayan Publik Me...  non_clickbait\n",
       "2025  Satgas: Jakarta dan Jabar Penyumbang Kasus Cov...      clickbait\n",
       "2026  Awal Mula Varian Baru Virus Corona Masuk ke In...  non_clickbait\n",
       "2027  Jokowi Sebut Pengangguran di Indonesia Hampir ...  non_clickbait\n",
       "2028                                                NaN            NaN\n",
       "\n",
       "[2029 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Limit data to Judul & Category columns\n",
    "news = news[['judul', 'kategori']]\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "#------------------\n",
    "\n",
    "import string\n",
    "import regex as re\n",
    "\n",
    "#Case Folding\n",
    "def to_lower(text):\n",
    "    return str(text).lower()\n",
    "\n",
    "#Remove Punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('','',string.punctuation)) #Search for punctuation in text, then translate it to ''\n",
    "\n",
    "\n",
    "def remove_punctuation2(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation]) #Search for char in text, if char not in string.punctuation, then join it\n",
    "\n",
    "#Remove Number\n",
    "def remove_number(text):\n",
    "    return ''.join([char for char in text if not char.isdigit()]) #Search for char in text, if char not in digit, then join it\n",
    "\n",
    "#Remove Whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "#Remove Whitespace multiple inside text\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text) #\\s+ means search for whitespace and replace with ' '\n",
    "\n",
    "#Remove Single Char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text) #\\b means search for single char and replace with ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Master \n",
    "#-----------\n",
    "\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword_1 = stopword_factory.get_stop_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "#Get Stopword from URL\n",
    "stopword_url = 'https://raw.githubusercontent.com/masdevid/ID-Stopwords/master/id.stopwords.02.01.2016.txt'\n",
    "\n",
    "def get_stopword(stopword_url):\n",
    "    stopwords = requests.get(stopword_url)\n",
    "    return stopwords.text\n",
    "\n",
    "stopwords_2 = get_stopword(stopword_url)\n",
    "stopwords_2 = [stopwords_2.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Stopword\n",
    "stopwords_all = stopword_1 + stopwords_2\n",
    "\n",
    "\n",
    "#Remove stopword function\n",
    "def remove_stopword(text):\n",
    "    text = [word for word in text.split() if word not in stopwords_all]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemma\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "#Create stemmer\n",
    "stem_factory = StemmerFactory().create_stemmer()\n",
    "\n",
    "def lemma(text):\n",
    "    return \" \".join([stem_factory.stem(word) for word in text.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile all preprocessing\n",
    "def prepro(text):\n",
    "    pre = to_lower(text)\n",
    "    pre = remove_punctuation(pre)\n",
    "    pre = remove_number(pre)\n",
    "    pre = remove_whitespace_LT(pre)\n",
    "    pre = remove_whitespace_multiple(pre)\n",
    "    pre = remove_singl_char(pre)\n",
    "    pre = remove_stopword(pre)\n",
    "    pre = lemma(pre)\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul</th>\n",
       "      <th>kategori</th>\n",
       "      <th>judul_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baekhyun EXO Berikan Contoh Baik Pencegahan Vi...</td>\n",
       "      <td>non_clickbait</td>\n",
       "      <td>baekhyun exo ikan contoh baik cegah virus coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lee Seung Gi Akhirnya Beri Kabar Setelah Dicar...</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>lee seung gi akhir beri kabar cari banyak oran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UPDATE: Jaejoong JYJ Ngaku Kena Corona, Idol K...</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>update jaejoong jyj ngaku kena corona idol kpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Virus Corona Masuk ke Korea, Kenapa Banyak Ora...</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>virus corona masuk korea banyak orang cari lee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16 Seleb yang Lagunya Bertema Virus Corona, Ad...</td>\n",
       "      <td>non_clickbait</td>\n",
       "      <td>seleb lagu tema virus corona bimbo rhoma irama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>Kemenkes Permudah Lansia dan Pelayan Publik Me...</td>\n",
       "      <td>non_clickbait</td>\n",
       "      <td>kemenkes mudah lansia layan publik dapat vaksin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>Satgas: Jakarta dan Jabar Penyumbang Kasus Cov...</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>satgas jakarta jabar sumbang kasus covid banya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>Awal Mula Varian Baru Virus Corona Masuk ke In...</td>\n",
       "      <td>non_clickbait</td>\n",
       "      <td>awal mula varian baru virus corona masuk indon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>Jokowi Sebut Pengangguran di Indonesia Hampir ...</td>\n",
       "      <td>non_clickbait</td>\n",
       "      <td>jokowi sebut anggur indonesia hampir juta akib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2029 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  judul       kategori  \\\n",
       "0     Baekhyun EXO Berikan Contoh Baik Pencegahan Vi...  non_clickbait   \n",
       "1     Lee Seung Gi Akhirnya Beri Kabar Setelah Dicar...      clickbait   \n",
       "2     UPDATE: Jaejoong JYJ Ngaku Kena Corona, Idol K...      clickbait   \n",
       "3     Virus Corona Masuk ke Korea, Kenapa Banyak Ora...      clickbait   \n",
       "4     16 Seleb yang Lagunya Bertema Virus Corona, Ad...  non_clickbait   \n",
       "...                                                 ...            ...   \n",
       "2024  Kemenkes Permudah Lansia dan Pelayan Publik Me...  non_clickbait   \n",
       "2025  Satgas: Jakarta dan Jabar Penyumbang Kasus Cov...      clickbait   \n",
       "2026  Awal Mula Varian Baru Virus Corona Masuk ke In...  non_clickbait   \n",
       "2027  Jokowi Sebut Pengangguran di Indonesia Hampir ...  non_clickbait   \n",
       "2028                                                NaN            NaN   \n",
       "\n",
       "                                     judul_preprocessed  \n",
       "0     baekhyun exo ikan contoh baik cegah virus coro...  \n",
       "1     lee seung gi akhir beri kabar cari banyak oran...  \n",
       "2     update jaejoong jyj ngaku kena corona idol kpo...  \n",
       "3     virus corona masuk korea banyak orang cari lee...  \n",
       "4        seleb lagu tema virus corona bimbo rhoma irama  \n",
       "...                                                 ...  \n",
       "2024    kemenkes mudah lansia layan publik dapat vaksin  \n",
       "2025  satgas jakarta jabar sumbang kasus covid banya...  \n",
       "2026  awal mula varian baru virus corona masuk indon...  \n",
       "2027  jokowi sebut anggur indonesia hampir juta akib...  \n",
       "2028                                                nan  \n",
       "\n",
       "[2029 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['judul_preprocessed'] = news['judul'].apply(prepro)\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Lemmatization & Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Lemmatization: rever the words back to dictionary form\n",
    "    \n",
    "    caring - cares - cared - caringly - carefully --> care - care - care - caringly - carefully\n",
    "\n",
    "- Stemming: heuristic process that removes the ends of words to root form of a word\n",
    "\n",
    "    caring - cares - cared - caringly - carefully --> care - care - care - care - care\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemma\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "#Create stemmer\n",
    "stem_factory = StemmerFactory().create_stemmer()\n",
    "\n",
    "def lemma(text):\n",
    "    return \" \".join([stem_factory.stem(word) for word in text.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jokowi Tetapkan Pembatasan Sosial Berskala Besar Hadapi Corona Covid-19, Apa Maksudnya?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'jokowi tetap batas sosial skala besar hadap corona covid apa maksud'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "display(news['judul'][99])\n",
    "lemma(news['judul_preprocessed'][99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List down all words in a document and count the frequency of each word\n",
    "- Tokenization : Splitting a string into a list of words\n",
    "- Vocabulary building: Collecting a list of all words\n",
    "- Vectorization: Counting the frequency of each word in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each unique words are compiled to columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF (Term Frequency) : Number of times a word appears in a document\n",
    "\n",
    "IDF (Inverse Document Frequency) : Inverse of the number of documents in which the word appears\n",
    "\n",
    "**TF-IDF (Term Frequency - Inverse Document Frequency) : TF * IDF**\n",
    "\n",
    "Rescale features by how informative we expect them to be\n",
    "-  Give high weight to any term appear often in particular document, not in many documents\n",
    "-  tfidf(word, doc) = tf(word) logn((N+1)/(Nw+1)) + 1, with\n",
    "    -  tf(word, doc): term freq of certain word of document\n",
    "    -  Nw: number of doc where the words appear\n",
    "    -  N: number of doc in training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Multiple Words: n-Grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bad\n",
    "- not bad --> not_bad \n",
    "\n",
    "if n-gram = (1,1) then\n",
    "- bad\n",
    "- not\n",
    "\n",
    "if n-gram = (1,2) then\n",
    "- bad not\n",
    "- not bad\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "\n",
    "X = news['judul_preprocessed']\n",
    "y = np.where(news['kategori'] == 'clickbait', 1, 0)\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1623, 12113)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF IDF Converter\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "Xtrain_tfidf = tfidf.fit_transform(Xtrain)\n",
    "Xtrain_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1623x12113 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 26461 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aa gym</th>\n",
       "      <th>aa umbara</th>\n",
       "      <th>aali</th>\n",
       "      <th>aali malah</th>\n",
       "      <th>ab</th>\n",
       "      <th>ab bagaimana</th>\n",
       "      <th>abai</th>\n",
       "      <th>abai corona</th>\n",
       "      <th>abai pasien</th>\n",
       "      <th>...</th>\n",
       "      <th>zubir tak</th>\n",
       "      <th>zubir umum</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuckerberg balik</th>\n",
       "      <th>zulkifli</th>\n",
       "      <th>zulkifli sebut</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zumba medan</th>\n",
       "      <th>zumi</th>\n",
       "      <th>zumi zola</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1623 rows × 12113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aa gym  aa umbara  aali  aali malah   ab  ab bagaimana  abai  \\\n",
       "0     0.0     0.0        0.0   0.0         0.0  0.0           0.0   0.0   \n",
       "1     0.0     0.0        0.0   0.0         0.0  0.0           0.0   0.0   \n",
       "2     0.0     0.0        0.0   0.0         0.0  0.0           0.0   0.0   \n",
       "3     0.0     0.0        0.0   0.0         0.0  0.0           0.0   0.0   \n",
       "4     0.0     0.0        0.0   0.0         0.0  0.0           0.0   0.0   \n",
       "...   ...     ...        ...   ...         ...  ...           ...   ...   \n",
       "1618  0.0     0.0        0.0   0.0         0.0  0.0           0.0   0.0   \n",
       "1619  0.0     0.0        0.0   0.0         0.0  0.0           0.0   0.0   \n",
       "1620  0.0     0.0        0.0   0.0         0.0  0.0           0.0   0.0   \n",
       "1621  0.0     0.0        0.0   0.0         0.0  0.0           0.0   0.0   \n",
       "1622  0.0     0.0        0.0   0.0         0.0  0.0           0.0   0.0   \n",
       "\n",
       "      abai corona  abai pasien  ...  zubir tak  zubir umum  zuckerberg  \\\n",
       "0             0.0          0.0  ...        0.0         0.0         0.0   \n",
       "1             0.0          0.0  ...        0.0         0.0         0.0   \n",
       "2             0.0          0.0  ...        0.0         0.0         0.0   \n",
       "3             0.0          0.0  ...        0.0         0.0         0.0   \n",
       "4             0.0          0.0  ...        0.0         0.0         0.0   \n",
       "...           ...          ...  ...        ...         ...         ...   \n",
       "1618          0.0          0.0  ...        0.0         0.0         0.0   \n",
       "1619          0.0          0.0  ...        0.0         0.0         0.0   \n",
       "1620          0.0          0.0  ...        0.0         0.0         0.0   \n",
       "1621          0.0          0.0  ...        0.0         0.0         0.0   \n",
       "1622          0.0          0.0  ...        0.0         0.0         0.0   \n",
       "\n",
       "      zuckerberg balik  zulkifli  zulkifli sebut  zumba  zumba medan  zumi  \\\n",
       "0                  0.0       0.0             0.0    0.0          0.0   0.0   \n",
       "1                  0.0       0.0             0.0    0.0          0.0   0.0   \n",
       "2                  0.0       0.0             0.0    0.0          0.0   0.0   \n",
       "3                  0.0       0.0             0.0    0.0          0.0   0.0   \n",
       "4                  0.0       0.0             0.0    0.0          0.0   0.0   \n",
       "...                ...       ...             ...    ...          ...   ...   \n",
       "1618               0.0       0.0             0.0    0.0          0.0   0.0   \n",
       "1619               0.0       0.0             0.0    0.0          0.0   0.0   \n",
       "1620               0.0       0.0             0.0    0.0          0.0   0.0   \n",
       "1621               0.0       0.0             0.0    0.0          0.0   0.0   \n",
       "1622               0.0       0.0             0.0    0.0          0.0   0.0   \n",
       "\n",
       "      zumi zola  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "...         ...  \n",
       "1618        0.0  \n",
       "1619        0.0  \n",
       "1620        0.0  \n",
       "1621        0.0  \n",
       "1622        0.0  \n",
       "\n",
       "[1623 rows x 12113 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "df_tfidf = pd.DataFrame(Xtrain_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(ngram_range=(1, 2))),\n",
       "                (&#x27;clf&#x27;, DecisionTreeClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(ngram_range=(1, 2))),\n",
       "                (&#x27;clf&#x27;, DecisionTreeClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 2))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect', TfidfVectorizer(ngram_range=(1, 2))),\n",
       "                ('clf', DecisionTreeClassifier())])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF IDF Converter\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "#Pipeline\n",
    "model_pipe = Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "model_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(ngram_range=(1, 2))),\n",
       "                (&#x27;clf&#x27;, DecisionTreeClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(ngram_range=(1, 2))),\n",
       "                (&#x27;clf&#x27;, DecisionTreeClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 2))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect', TfidfVectorizer(ngram_range=(1, 2))),\n",
       "                ('clf', DecisionTreeClassifier())])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply model\n",
    "\n",
    "model_pipe.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_pipe.predict(Xtest)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[205  81]\n",
      " [ 63  57]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       286\n",
      "           1       0.41      0.47      0.44       120\n",
      "\n",
      "    accuracy                           0.65       406\n",
      "   macro avg       0.59      0.60      0.59       406\n",
      "weighted avg       0.66      0.65      0.65       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.645320197044335\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy : ', accuracy_score(ytest, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_headline = [\n",
    "'Ketika Jokowi Minta Masyarakat Tak Takut Corona, Ini Kata Dokter Tirta' ,\n",
    "'Viral Video Pembukaan Mal di India Diserbu hingga Dijarah Pengunjung, Warga Ambil Makanan Tak Bayar'\n",
    "\n",
    ",'Siaran Pers: Kemenparekraf Perkuat Pasar Asia Pasifik Melalui PATA Travel Mart di India'\n",
    ",'GEGER: Kemenparekraf VIRAL Perkuat KACAU Pasar Asia HEBOH Pasifik Melalui PATA Travel Mart di India'\n",
    "\n",
    ",'Heboh Xiaomi dan Vivo Dituduh Tebar Propaganda China'\n",
    ",'Cara Bikin Judul Clickbait Lebih Menarik, tapi Tetap Aman'\n",
    "\n",
    ",'Elite Politik Yakin PM Ardern Mundur karena Ancaman dan Pelecehan'\n",
    ",'Clickbait banget ini Elite Politik Yakin PM Ardern Mundur karena Ancaman dan Pelecehan',\n",
    "\n",
    "'Soal Dukun Cabul di Aceh Berjuluk Pesulap Hijau, Pesulap Merah Keberatan: Kacau Banget']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickbait_status = model_pipe.predict(news_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judul</th>\n",
       "      <th>Clickbait?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ketika Jokowi Minta Masyarakat Tak Takut Coron...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Viral Video Pembukaan Mal di India Diserbu hin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siaran Pers: Kemenparekraf Perkuat Pasar Asia ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GEGER: Kemenparekraf VIRAL Perkuat KACAU Pasar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heboh Xiaomi dan Vivo Dituduh Tebar Propaganda...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cara Bikin Judul Clickbait Lebih Menarik, tapi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elite Politik Yakin PM Ardern Mundur karena An...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Clickbait banget ini Elite Politik Yakin PM Ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Soal Dukun Cabul di Aceh Berjuluk Pesulap Hija...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Judul  Clickbait?\n",
       "0  Ketika Jokowi Minta Masyarakat Tak Takut Coron...           1\n",
       "1  Viral Video Pembukaan Mal di India Diserbu hin...           1\n",
       "2  Siaran Pers: Kemenparekraf Perkuat Pasar Asia ...           0\n",
       "3  GEGER: Kemenparekraf VIRAL Perkuat KACAU Pasar...           1\n",
       "4  Heboh Xiaomi dan Vivo Dituduh Tebar Propaganda...           0\n",
       "5  Cara Bikin Judul Clickbait Lebih Menarik, tapi...           1\n",
       "6  Elite Politik Yakin PM Ardern Mundur karena An...           0\n",
       "7  Clickbait banget ini Elite Politik Yakin PM Ar...           1\n",
       "8  Soal Dukun Cabul di Aceh Berjuluk Pesulap Hija...           1"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Judul': news_headline, 'Clickbait?': clickbait_status})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
